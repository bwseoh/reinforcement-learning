{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-14 12:57:49,631] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari iamges. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84, 1] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "        # Our input are 4 RGB frames of shape 160, 160 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three convolutional layers\n",
    "        conv1 = tf.contrib.layers.conv2d(\n",
    "            X, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        conv2 = tf.contrib.layers.conv2d(\n",
    "            conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        conv3 = tf.contrib.layers.conv2d(\n",
    "            conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # Fully connected layers\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calcualte the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05440644  0.02550881  0.          0.05310513]\n",
      " [ 0.05440644  0.02550881  0.          0.05310513]]\n",
      "99.2156\n"
     ]
    }
   ],
   "source": [
    "# For Testing....\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "e = Estimator(scope=\"test\")\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    observation_p = sp.process(sess, observation)\n",
    "    observation = np.stack([observation_p] * 4, axis=2)\n",
    "    observations = np.array([observation] * 2)\n",
    "    \n",
    "    # Test Prediction\n",
    "    print(e.predict(sess, observations))\n",
    "\n",
    "    # Test training step\n",
    "    y = np.array([10.0, 10.0])\n",
    "    a = np.array([1, 3])\n",
    "    print(e.update(sess, observations, a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_model_parameters(sess, estimator1, estimator2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sample when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Lambda time discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    random_state = env.reset()\n",
    "    random_state = state_processor.process(sess, random_state)\n",
    "    random_state = np.stack([random_state] * 4, axis=2)\n",
    "\n",
    "    for i in range(replay_memory_init_size):\n",
    "        # TODO: Populate replay memory!\n",
    "        random_action = np.random.randint(low=0, high=env.action_space.n)\n",
    "        \n",
    "        random_next_state = np.zeros(random_state.shape)\n",
    "        \n",
    "        random_observation, random_reward, random_done, _ = env.step(random_action)\n",
    "\n",
    "        # env.render() - Uncomment this if you want to see the actions rendered\n",
    "        \n",
    "        # Add the last 3 frames from the previous state\n",
    "        # Current State = An array of last 4 frames\n",
    "        random_next_state[:,:,0:3] = random_state[:,:,-3:]\n",
    "        random_next_state[:,:,3] = state_processor.process(sess, random_observation)\n",
    "        \n",
    "        random_new_transition = Transition(\n",
    "            random_state,\n",
    "            random_action,\n",
    "            random_reward,\n",
    "            random_next_state,\n",
    "            random_done\n",
    "        )\n",
    "        \n",
    "        replay_memory.append(random_new_transition)\n",
    "        \n",
    "        if random_done:\n",
    "            random_state = env.reset()\n",
    "            random_state = state_processor.process(sess, random_state)\n",
    "            random_state = np.stack([random_state] * 4, axis=2)\n",
    "        else:\n",
    "            random_state = random_next_state\n",
    "\n",
    "    # Record videos\n",
    "    env = Monitor(\n",
    "            env,\n",
    "            directory=monitor_path,\n",
    "            resume=True,\n",
    "            video_callable=lambda count: count % record_video_every == 0\n",
    "    )\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 4, axis=2) # I think axis=0 would have been more intuitive\n",
    "        loss = None\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # Add epsilon to Tensorboard\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=epsilon, tag=\"epsilon\")\n",
    "            q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "\n",
    "            # TODO: Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                copy_model_parameters(sess, q_estimator, target_estimator)\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step in the environment\n",
    "            # TODO: Implement!\n",
    "            action = np.random.choice(\n",
    "                VALID_ACTIONS,\n",
    "                p=policy(sess, state, epsilon)\n",
    "            )\n",
    "            \n",
    "            next_state = np.zeros(state.shape)\n",
    "        \n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            # env.render() - Uncomment this if you want to see the actions rendered\n",
    "        \n",
    "            # Add the last 3 frames from the previous state\n",
    "            # Current State = An array of last 4 frames\n",
    "            next_state[:,:,0:3] = state[:,:,-3:]\n",
    "            next_state[:,:,3] = state_processor.process(sess, observation)\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # TODO: Save transition to replay memory\n",
    "            new_transition = Transition(state, action, reward, next_state, done)\n",
    "            replay_memory.append(new_transition)\n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # TODO: Sample a minibatch from the replay memory\n",
    "            # TODO: Calculate q values and targets\n",
    "            # TODO Perform gradient descent update\n",
    "            \n",
    "            # So we are fitting with a minibatch at every step of an episode?\n",
    "            if done:\n",
    "                # Let's finish up the episode\n",
    "                break\n",
    "            else:\n",
    "                # Let's sample a minibatch.\n",
    "                mini_batch = random.sample(replay_memory, batch_size)\n",
    "                \n",
    "                mini_batch_states = np.zeros(\n",
    "                    (batch_size, state.shape[0], state.shape[1], state.shape[2])\n",
    "                )\n",
    "                mini_batch_actions = np.zeros(batch_size)\n",
    "                mini_batch_rewards = np.zeros(batch_size)\n",
    "                \n",
    "                for m, transition in enumerate(mini_batch):\n",
    "                    mini_batch_states[m] = transition.state\n",
    "                    mini_batch_actions[m] = transition.action\n",
    "                    mini_batch_rewards[m] = transition.reward\n",
    "                    \n",
    "                target = mini_batch_rewards + \\\n",
    "                discount_factor * np.nanmax(target_estimator.predict(sess, mini_batch_states), axis=1)\n",
    "\n",
    "                # Now updates are done in batches\n",
    "                loss = q_estimator.update(\n",
    "                    sess,\n",
    "                    mini_batch_states,\n",
    "                    mini_batch_actions,\n",
    "                    # Targets\n",
    "                    target\n",
    "                )\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(\n",
    "            simple_value=stats.episode_rewards[i_episode],\n",
    "            node_name=\"episode_reward\", tag=\"episode_reward\"\n",
    "        )\n",
    "        episode_summary.value.add(\n",
    "            simple_value=stats.episode_lengths[i_episode],\n",
    "            node_name=\"episode_length\",\n",
    "            tag=\"episode_length\"\n",
    "        )\n",
    "\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "        q_estimator.summary_writer.flush()\n",
    "\n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    env.close()\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-14 13:02:16,380] Starting new video recorder writing to /Users/se9/Documents/git/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.69736.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 231 (231) @ Episode 1/10000, loss: 0.0007766829803586006\n",
      "Episode Reward: 1.0\n",
      "Step 179 (410) @ Episode 2/10000, loss: 0.00066443427931517367\n",
      "Episode Reward: 0.0\n",
      "Step 234 (644) @ Episode 3/10000, loss: 0.00066007452551275494\n",
      "Episode Reward: 1.0\n",
      "Step 175 (819) @ Episode 4/10000, loss: 0.00062022218480706213\n",
      "Episode Reward: 0.0\n",
      "Step 277 (1096) @ Episode 5/10000, loss: 0.03189640864729881615\n",
      "Episode Reward: 2.0\n",
      "Step 229 (1325) @ Episode 6/10000, loss: 0.03432178869843483658\n",
      "Episode Reward: 1.0\n",
      "Step 237 (1562) @ Episode 7/10000, loss: 0.00040595239261165263\n",
      "Episode Reward: 1.0\n",
      "Step 248 (1810) @ Episode 8/10000, loss: 0.03020681999623775552\n",
      "Episode Reward: 1.0\n",
      "Step 180 (1990) @ Episode 9/10000, loss: 5.155501276021823e-056\n",
      "Episode Reward: 0.0\n",
      "Step 220 (2210) @ Episode 10/10000, loss: 9.694894833955914e-055\n",
      "Episode Reward: 1.0\n",
      "Step 211 (2421) @ Episode 11/10000, loss: 0.00012025394971715286\n",
      "Episode Reward: 1.0\n",
      "Step 186 (2607) @ Episode 12/10000, loss: 0.00016789403161965317\n",
      "Episode Reward: 0.0\n",
      "Step 381 (2988) @ Episode 13/10000, loss: 5.7144061429426074e-05\n",
      "Episode Reward: 4.0\n",
      "Step 181 (3169) @ Episode 14/10000, loss: 2.1881052816752344e-05\n",
      "Episode Reward: 0.0\n",
      "Step 332 (3501) @ Episode 15/10000, loss: 1.774902193574235e-055\n",
      "Episode Reward: 3.0\n",
      "Step 215 (3716) @ Episode 16/10000, loss: 2.869265154004097e-055\n",
      "Episode Reward: 1.0\n",
      "Step 303 (4019) @ Episode 17/10000, loss: 8.613089448772371e-055\n",
      "Episode Reward: 2.0\n",
      "Step 217 (4236) @ Episode 18/10000, loss: 7.509170973207802e-055\n",
      "Episode Reward: 1.0\n",
      "Step 234 (4470) @ Episode 19/10000, loss: 0.03115083463490009365\n",
      "Episode Reward: 1.0\n",
      "Step 270 (4740) @ Episode 20/10000, loss: 5.3574200137518346e-05\n",
      "Episode Reward: 2.0\n",
      "Step 208 (4948) @ Episode 21/10000, loss: 0.03083548508584499405\n",
      "Episode Reward: 1.0\n",
      "Step 196 (5144) @ Episode 22/10000, loss: 8.986941247712821e-053\n",
      "Episode Reward: 1.0\n",
      "Step 176 (5320) @ Episode 23/10000, loss: 5.7552715588826686e-05\n",
      "Episode Reward: 0.0\n",
      "Step 261 (5581) @ Episode 24/10000, loss: 6.323262823570985e-065\n",
      "Episode Reward: 2.0\n",
      "Step 187 (5768) @ Episode 25/10000, loss: 3.885340265696868e-055\n",
      "Episode Reward: 0.0\n",
      "Step 177 (5945) @ Episode 26/10000, loss: 7.62311537982896e-0505\n",
      "Episode Reward: 0.0\n",
      "Step 213 (6158) @ Episode 27/10000, loss: 5.400740064942511e-065\n",
      "Episode Reward: 1.0\n",
      "Step 366 (6524) @ Episode 28/10000, loss: 7.3403966780460905e-06\n",
      "Episode Reward: 3.0\n",
      "Step 196 (6720) @ Episode 29/10000, loss: 2.6552594135864638e-05\n",
      "Episode Reward: 0.0\n",
      "Step 277 (6997) @ Episode 30/10000, loss: 5.8544181229081005e-05\n",
      "Episode Reward: 2.0\n",
      "Step 226 (7223) @ Episode 31/10000, loss: 4.645734588848427e-055\n",
      "Episode Reward: 1.0\n",
      "Step 354 (7577) @ Episode 32/10000, loss: 3.051505882467609e-055\n",
      "Episode Reward: 3.0\n",
      "Step 475 (8052) @ Episode 33/10000, loss: 0.03109339810907840755\n",
      "Episode Reward: 6.0\n",
      "Step 171 (8223) @ Episode 34/10000, loss: 4.734991671284661e-055\n",
      "Episode Reward: 0.0\n",
      "Step 244 (8467) @ Episode 35/10000, loss: 0.03155424073338508605\n",
      "Episode Reward: 2.0\n",
      "Step 244 (8711) @ Episode 36/10000, loss: 1.7153073713416234e-05\n",
      "Episode Reward: 2.0\n",
      "Step 317 (9028) @ Episode 37/10000, loss: 2.065389526251238e-055\n",
      "Episode Reward: 2.0\n",
      "Step 174 (9202) @ Episode 38/10000, loss: 0.00012315330968704075\n",
      "Episode Reward: 0.0\n",
      "Step 181 (9383) @ Episode 39/10000, loss: 2.266636147396639e-055\n",
      "Episode Reward: 0.0\n",
      "Step 270 (9653) @ Episode 40/10000, loss: 5.4757754696765915e-05\n",
      "Episode Reward: 2.0\n",
      "Step 196 (9849) @ Episode 41/10000, loss: 1.932315353769809e-055\n",
      "Episode Reward: 0.0\n",
      "Step 208 (10057) @ Episode 42/10000, loss: 0.00023547171440441161\n",
      "Episode Reward: 1.0\n",
      "Step 230 (10287) @ Episode 43/10000, loss: 2.673938070074655e-055\n",
      "Episode Reward: 1.0\n",
      "Step 237 (10524) @ Episode 44/10000, loss: 1.8272317902301438e-05\n",
      "Episode Reward: 1.0\n",
      "Step 252 (10776) @ Episode 45/10000, loss: 7.563171675428748e-051\n",
      "Episode Reward: 2.0\n",
      "Step 359 (11135) @ Episode 46/10000, loss: 3.884565012413077e-055\n",
      "Episode Reward: 4.0\n",
      "Step 321 (11456) @ Episode 47/10000, loss: 5.672369297826663e-065\n",
      "Episode Reward: 3.0\n",
      "Step 171 (11627) @ Episode 48/10000, loss: 4.137857467867434e-055\n",
      "Episode Reward: 0.0\n",
      "Step 189 (11816) @ Episode 49/10000, loss: 0.00012712494935840368\n",
      "Episode Reward: 0.0\n",
      "Step 221 (12037) @ Episode 50/10000, loss: 5.2673927712021396e-05\n",
      "Episode Reward: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-14 13:42:01,035] Starting new video recorder writing to /Users/se9/Documents/git/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.69736.video000050.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 176 (12213) @ Episode 51/10000, loss: 4.682037251768634e-055\n",
      "Episode Reward: 0.0\n",
      "Step 193 (12406) @ Episode 52/10000, loss: 3.390685833437601e-065\n",
      "Episode Reward: 0.0\n",
      "Step 266 (12672) @ Episode 53/10000, loss: 9.126549412030727e-055\n",
      "Episode Reward: 2.0\n",
      "Step 181 (12853) @ Episode 54/10000, loss: 6.708532600896433e-056\n",
      "Episode Reward: 0.0\n",
      "Step 279 (13132) @ Episode 55/10000, loss: 7.266087777679786e-065\n",
      "Episode Reward: 2.0\n",
      "Step 174 (13306) @ Episode 56/10000, loss: 2.9182532671256922e-05\n",
      "Episode Reward: 0.0\n",
      "Step 270 (13576) @ Episode 57/10000, loss: 0.03068785928189754582\n",
      "Episode Reward: 2.0\n",
      "Step 172 (13748) @ Episode 58/10000, loss: 4.617425292963162e-055\n",
      "Episode Reward: 0.0\n",
      "Step 310 (14058) @ Episode 59/10000, loss: 6.969897367525846e-055\n",
      "Episode Reward: 3.0\n",
      "Step 230 (14288) @ Episode 60/10000, loss: 1.5868656191742048e-05\n",
      "Episode Reward: 1.0\n",
      "Step 274 (14562) @ Episode 61/10000, loss: 3.367868339410052e-055\n",
      "Episode Reward: 2.0\n",
      "Step 239 (14801) @ Episode 62/10000, loss: 1.842848905653227e-055\n",
      "Episode Reward: 1.0\n",
      "Step 189 (14990) @ Episode 63/10000, loss: 0.03101331181824207351\n",
      "Episode Reward: 0.0\n",
      "Step 211 (15201) @ Episode 64/10000, loss: 1.6803020116640255e-05\n",
      "Episode Reward: 1.0\n",
      "Step 283 (15484) @ Episode 65/10000, loss: 0.03096043877303600355\n",
      "Episode Reward: 2.0\n",
      "Step 279 (15763) @ Episode 66/10000, loss: 6.302616384346038e-058\n",
      "Episode Reward: 2.0\n",
      "Step 350 (16113) @ Episode 67/10000, loss: 3.131185076199472e-055\n",
      "Episode Reward: 3.0\n",
      "Step 409 (16522) @ Episode 68/10000, loss: 0.00012701848754659295\n",
      "Episode Reward: 4.0\n",
      "Step 182 (16704) @ Episode 69/10000, loss: 5.071188206784427e-055\n",
      "Episode Reward: 0.0\n",
      "Step 248 (16952) @ Episode 70/10000, loss: 2.732692883000709e-055\n",
      "Episode Reward: 2.0\n",
      "Step 174 (17126) @ Episode 71/10000, loss: 6.018599742674269e-054\n",
      "Episode Reward: 0.0\n",
      "Step 346 (17472) @ Episode 72/10000, loss: 1.0937450497294776e-05\n",
      "Episode Reward: 3.0\n",
      "Step 167 (17639) @ Episode 73/10000, loss: 4.768131475429982e-055\n",
      "Episode Reward: 0.0\n",
      "Step 232 (17871) @ Episode 74/10000, loss: 0.0311506949365139e-05\n",
      "Episode Reward: 1.0\n",
      "Step 169 (18040) @ Episode 75/10000, loss: 5.057192174717784e-055\n",
      "Episode Reward: 0.0\n",
      "Step 251 (18291) @ Episode 76/10000, loss: 0.00014491885667666793\n",
      "Episode Reward: 1.0\n",
      "Step 368 (18659) @ Episode 77/10000, loss: 2.2840684323455207e-05\n",
      "Episode Reward: 4.0\n",
      "Step 178 (18837) @ Episode 78/10000, loss: 0.00017259008018299937\n",
      "Episode Reward: 0.0\n",
      "Step 167 (19004) @ Episode 79/10000, loss: 8.674839773448184e-057\n",
      "Episode Reward: 0.0\n",
      "Step 243 (19247) @ Episode 80/10000, loss: 2.2572523448616266e-05\n",
      "Episode Reward: 1.0\n",
      "Step 257 (19504) @ Episode 81/10000, loss: 6.875547114759684e-055\n",
      "Episode Reward: 2.0\n",
      "Step 311 (19815) @ Episode 82/10000, loss: 0.03097508102655410855\n",
      "Episode Reward: 3.0\n",
      "Step 233 (20048) @ Episode 83/10000, loss: 7.895305316196755e-056\n",
      "Episode Reward: 1.0\n",
      "Step 180 (20228) @ Episode 84/10000, loss: 4.964951131114503e-065\n",
      "Episode Reward: 0.0\n",
      "Step 288 (20516) @ Episode 85/10000, loss: 6.577788735739887e-052\n",
      "Episode Reward: 2.0\n",
      "Step 230 (20746) @ Episode 86/10000, loss: 0.00025158969219774014\n",
      "Episode Reward: 1.0\n",
      "Step 233 (20979) @ Episode 87/10000, loss: 2.9836610337952152e-05\n",
      "Episode Reward: 1.0\n",
      "Step 215 (21194) @ Episode 88/10000, loss: 0.00029567824094556276\n",
      "Episode Reward: 1.0\n",
      "Step 182 (21376) @ Episode 89/10000, loss: 0.00019751963554881513\n",
      "Episode Reward: 0.0\n",
      "Step 337 (21713) @ Episode 90/10000, loss: 4.944257307215594e-054\n",
      "Episode Reward: 3.0\n",
      "Step 215 (21928) @ Episode 91/10000, loss: 9.560903708916157e-054\n",
      "Episode Reward: 1.0\n",
      "Step 243 (22171) @ Episode 92/10000, loss: 7.014987932052463e-052\n",
      "Episode Reward: 1.0\n",
      "Step 199 (22370) @ Episode 93/10000, loss: 0.02968527562916279055\n",
      "Episode Reward: 1.0\n",
      "Step 305 (22675) @ Episode 94/10000, loss: 4.3904401536565274e-05\n",
      "Episode Reward: 2.0\n",
      "Step 171 (22846) @ Episode 95/10000, loss: 2.1805601136293262e-05\n",
      "Episode Reward: 0.0\n",
      "Step 301 (23147) @ Episode 96/10000, loss: 6.816492714278866e-065\n",
      "Episode Reward: 3.0\n",
      "Step 283 (23430) @ Episode 97/10000, loss: 1.885462916106917e-055\n",
      "Episode Reward: 2.0\n",
      "Step 275 (23705) @ Episode 98/10000, loss: 0.03051307797431945805\n",
      "Episode Reward: 2.0\n",
      "Step 236 (23941) @ Episode 99/10000, loss: 2.118141856044531e-055\n",
      "Episode Reward: 1.0\n",
      "Step 207 (24148) @ Episode 100/10000, loss: 8.421111488132738e-066\n",
      "Episode Reward: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-14 14:21:47,570] Starting new video recorder writing to /Users/se9/Documents/git/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.69736.video000100.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 175 (24323) @ Episode 101/10000, loss: 6.405471503967419e-051\n",
      "Episode Reward: 0.0\n",
      "Step 240 (24563) @ Episode 102/10000, loss: 6.217000191099942e-059\n",
      "Episode Reward: 1.0\n",
      "Step 269 (24832) @ Episode 103/10000, loss: 1.706360126263462e-055\n",
      "Episode Reward: 2.0\n",
      "Step 281 (25113) @ Episode 104/10000, loss: 3.6394179915077984e-05\n",
      "Episode Reward: 2.0\n",
      "Step 172 (25285) @ Episode 105/10000, loss: 2.6845198590308428e-05\n",
      "Episode Reward: 0.0\n",
      "Step 218 (25503) @ Episode 106/10000, loss: 7.346178608713672e-055\n",
      "Episode Reward: 1.0\n",
      "Step 209 (25712) @ Episode 107/10000, loss: 6.518737063743174e-055\n",
      "Episode Reward: 1.0\n",
      "Step 267 (25979) @ Episode 108/10000, loss: 6.276512976910453e-065\n",
      "Episode Reward: 2.0\n",
      "Step 179 (26158) @ Episode 109/10000, loss: 1.59893388627097e-0505\n",
      "Episode Reward: 0.0\n",
      "Step 170 (26328) @ Episode 110/10000, loss: 0.00018768984591588378\n",
      "Episode Reward: 0.0\n",
      "Step 179 (26507) @ Episode 111/10000, loss: 2.7375655918149278e-05\n",
      "Episode Reward: 0.0\n",
      "Step 184 (26691) @ Episode 112/10000, loss: 5.626110214507207e-055\n",
      "Episode Reward: 0.0\n",
      "Step 175 (26866) @ Episode 113/10000, loss: 3.0941926524974406e-05\n",
      "Episode Reward: 0.0\n",
      "Step 167 (27033) @ Episode 114/10000, loss: 4.8073052312247455e-05\n",
      "Episode Reward: 0.0\n",
      "Step 323 (27356) @ Episode 115/10000, loss: 4.434444235812407e-065\n",
      "Episode Reward: 3.0\n",
      "Step 305 (27661) @ Episode 116/10000, loss: 6.861049769213423e-055\n",
      "Episode Reward: 2.0\n",
      "Step 188 (27849) @ Episode 117/10000, loss: 0.02942288853228092205\n",
      "Episode Reward: 0.0\n",
      "Step 208 (28057) @ Episode 118/10000, loss: 0.03073366172611713405\n",
      "Episode Reward: 1.0\n",
      "Step 305 (28362) @ Episode 119/10000, loss: 3.8327172660501674e-05\n",
      "Episode Reward: 2.0\n",
      "Step 223 (28585) @ Episode 120/10000, loss: 5.680261892848648e-058\n",
      "Episode Reward: 1.0\n",
      "Step 241 (28826) @ Episode 121/10000, loss: 6.868108903290704e-058\n",
      "Episode Reward: 1.0\n",
      "Step 268 (29094) @ Episode 122/10000, loss: 1.226403492182726e-055\n",
      "Episode Reward: 2.0\n",
      "Step 214 (29308) @ Episode 123/10000, loss: 0.00015405561134684835\n",
      "Episode Reward: 1.0\n",
      "Step 366 (29674) @ Episode 124/10000, loss: 9.876487456494942e-055\n",
      "Episode Reward: 3.0\n",
      "Step 220 (29894) @ Episode 125/10000, loss: 0.00029609751072712246"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a global step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=10000,\n",
    "                                    replay_memory_size=500000,\n",
    "                                    replay_memory_init_size=50000,\n",
    "                                    update_target_estimator_every=10000,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=500000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=32):\n",
    "\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
